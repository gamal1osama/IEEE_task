Random Forest is an ensemble learning method that combines multiple decision trees to improve
the accuracy and robustness of predictions. It is primarily used for classification and regression tasks

How it works:
	1. Bootstrapping:
		* Random Forest creates multiple decision trees by training them on random subsets of the original training data.
		  Each subset is generated by sampling the data with replacement (bootstrapping), meaning some data points might appear more than once in a single tree.
  	2. Random Feature Selection:
		* When splitting nodes, Random Forest randomly selects a subset of features, which promotes diversity among trees and enhances the model's generalization
		  by preventing similarity between them.
	3. Building Multiple Trees:
		* Each decision tree is built independently using the bootstrapped data and random features.
	4. Voting/Averaging :
		* For classification: After all the trees make their predictions, the final prediction is made by taking a majority vote from all the trees
		* For regression: The final prediction is the average of all the tree predictions

Advantages:
	1- Reduces overfitting
	2- Handles large datasets well
	3- Robust: It performs well even if some data features are missing or noisy